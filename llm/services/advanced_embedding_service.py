"""
Advanced Embedding Service —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Sentence Transformers

–î–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:
1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers
2. –í preprocessing_service.py –∑–∞–º–µ–Ω–∏—Ç–µ:
   self._generate_improved_embedding() ‚Üí self._generate_transformer_embedding()

–ú–æ–¥–µ–ª—å all-MiniLM-L6-v2:
- –†–∞–∑–º–µ—Ä: ~80MB
- –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: 384
- –ö–∞—á–µ—Å—Ç–≤–æ: –æ—Ç–ª–∏—á–Ω–æ–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
- –°–∫–æ—Ä–æ—Å—Ç—å: ~100 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π/—Å–µ–∫ –Ω–∞ CPU
"""

import json
from typing import List, Optional
import numpy as np

# –ò–º–ø–æ—Ä—Ç –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω sentence-transformers
try:
    from sentence_transformers import SentenceTransformer

    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:
    SENTENCE_TRANSFORMERS_AVAILABLE = False
    SentenceTransformer = None


class AdvancedEmbeddingService:
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö embeddings

    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ sentence-transformers
    –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """

    def __init__(
            self,
            model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
            cache_folder: Optional[str] = None
    ):
        """
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace
            cache_folder: –ü–∞–ø–∫–∞ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if not SENTENCE_TRANSFORMERS_AVAILABLE:
            raise ImportError(
                "sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. "
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install sentence-transformers"
            )

        self.model_name = model_name
        self._model: Optional[SentenceTransformer] = None
        self.cache_folder = cache_folder
        self.embedding_dim = 384  # –î–ª—è all-MiniLM-L6-v2

    @property
    def model(self) -> SentenceTransformer:
        """Lazy loading –º–æ–¥–µ–ª–∏"""
        if self._model is None:
            print(f"üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å {self.model_name}...")
            self._model = SentenceTransformer(
                self.model_name,
                cache_folder=self.cache_folder
            )
            print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        return self._model

    def generate_embedding(self, text: str) -> List[float]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ embedding

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏

        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 384
        """
        if not text or not text.strip():
            return [0.0] * self.embedding_dim

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embedding
        embedding = self.model.encode(
            text,
            normalize_embeddings=True,  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
            show_progress_bar=False
        )

        return embedding.tolist()

    def generate_embeddings_batch(
            self,
            texts: List[str],
            batch_size: int = 32
    ) -> List[List[float]]:
        """
        –ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è embeddings (—ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä—ë–º–æ–≤)

        Args:
            texts: –°–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–µ–∫—Ç–æ—Ä–æ–≤
        """
        if not texts:
            return []

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã
        valid_texts = [t if t and t.strip() else " " for t in texts]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –±–∞—Ç—á–µ–º
        embeddings = self.model.encode(
            valid_texts,
            batch_size=batch_size,
            normalize_embeddings=True,
            show_progress_bar=len(texts) > 100
        )

        return [emb.tolist() for emb in embeddings]

    def compute_similarity(
            self,
            embedding1: List[float],
            embedding2: List[float]
    ) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ cosine similarity –º–µ–∂–¥—É embedding

        –¢–∞–∫ –∫–∞–∫ embeddings –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã, —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ dot product
        """
        arr1 = np.array(embedding1)
        arr2 = np.array(embedding2)
        return float(np.dot(arr1, arr2))

    def find_most_similar(
            self,
            query_embedding: List[float],
            candidate_embeddings: List[List[float]],
            top_k: int = 10
    ) -> List[tuple]:
        """
        –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂–∏—Ö embeddings

        Args:
            query_embedding: Embedding –∑–∞–ø—Ä–æ—Å–∞
            candidate_embeddings: –°–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            top_k: –°–∫–æ–ª—å–∫–æ –≤–µ—Ä–Ω—É—Ç—å

        Returns:
            List[(index, similarity_score)]
        """
        query = np.array(query_embedding)
        candidates = np.array(candidate_embeddings)

        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ = dot product (—Ç–∞–∫ –∫–∞–∫ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)
        similarities = np.dot(candidates, query)

        # –¢–æ–ø-K –∏–Ω–¥–µ–∫—Å–æ–≤
        top_indices = np.argsort(similarities)[-top_k:][::-1]

        return [
            (int(idx), float(similarities[idx]))
            for idx in top_indices
        ]


class HybridEmbeddingService:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π —Å–µ—Ä–≤–∏—Å: –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç transformer embeddings —Å –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–º–∏

    –°—Ç—Ä–∞—Ç–µ–≥–∏—è:
    - –î–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ (preprocessing) –∏—Å–ø–æ–ª—å–∑—É–µ–º transformer
    - –î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ embeddings
    - Fallback –Ω–∞ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ –µ—Å–ª–∏ transformer –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
    """

    def __init__(
            self,
            use_transformers: bool = True,
            fallback_service=None
    ):
        """
        Args:
            use_transformers: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ transformer –º–æ–¥–µ–ª—å
            fallback_service: –°–µ—Ä–≤–∏—Å –¥–ª—è fallback (–∏–∑ improved_preprocessing)
        """
        self.use_transformers = use_transformers and SENTENCE_TRANSFORMERS_AVAILABLE
        self.fallback_service = fallback_service

        if self.use_transformers:
            self.transformer_service = AdvancedEmbeddingService()
        else:
            self.transformer_service = None

    def generate_embedding(self, text: str) -> List[float]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å fallback"""
        if self.use_transformers and self.transformer_service:
            try:
                return self.transformer_service.generate_embedding(text)
            except Exception as e:
                print(f"‚ö†Ô∏è Transformer failed: {e}, using fallback")
                if self.fallback_service:
                    return self.fallback_service._generate_improved_embedding(text)

        if self.fallback_service:
            return self.fallback_service._generate_improved_embedding(text)

        raise ValueError("No embedding service available")

    def generate_embeddings_batch(
            self,
            texts: List[str]
    ) -> List[List[float]]:
        """–ü–∞–∫–µ—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å fallback"""
        if self.use_transformers and self.transformer_service:
            try:
                return self.transformer_service.generate_embeddings_batch(texts)
            except Exception as e:
                print(f"‚ö†Ô∏è Transformer batch failed: {e}, using fallback")

        # Fallback: –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ –æ–¥–Ω–æ–º—É
        if self.fallback_service:
            return [
                self.fallback_service._generate_improved_embedding(text)
                for text in texts
            ]

        raise ValueError("No embedding service available")


# ============================================================================
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤ improved_preprocessing_service.py
# ============================================================================

"""
–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–±–∞–≤—å—Ç–µ –≤ improved_preprocessing_service.py:

1. –í __init__:

def __init__(self, data_adapter, use_transformer_embeddings: bool = False):
    self.data_adapter = data_adapter
    self.stop_words = {...}

    # –î–û–ë–ê–í–¨–¢–ï –≠–¢–û:
    self.use_transformer_embeddings = use_transformer_embeddings

    if use_transformer_embeddings:
        try:
            from .advanced_embedding_service import AdvancedEmbeddingService
            self.embedding_service = AdvancedEmbeddingService()
            print("‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è transformer embeddings")
        except ImportError:
            print("‚ö†Ô∏è sentence-transformers –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ")
            self.embedding_service = None
            self.use_transformer_embeddings = False
    else:
        self.embedding_service = None


2. –í –º–µ—Ç–æ–¥–µ preprocess_solution –∑–∞–º–µ–Ω–∏—Ç–µ:

# –ë—ã–ª–æ:
embedding = self._generate_improved_embedding(text)

# –°—Ç–∞–ª–æ:
if self.use_transformer_embeddings and self.embedding_service:
    embedding = self.embedding_service.generate_embedding(text)
else:
    embedding = self._generate_improved_embedding(text)


3. –î–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ laboratory_service.py:

# –í get_laboratory_service():
preprocessing_service = ImprovedPreprocessingService(
    data_adapter,
    use_transformer_embeddings=True  # –í–ö–õ–Æ–ß–ò–¢–¨ –ó–î–ï–°–¨
)
"""


# ============================================================================
# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
# ============================================================================

class EmbeddingMigrationHelper:
    """
    –ü–æ–º–æ—â–Ω–∏–∫ –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ —Å –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã—Ö –Ω–∞ transformer embeddings

    –ü–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –º–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏
    """

    def __init__(
            self,
            data_adapter,
            batch_size: int = 50
    ):
        self.data_adapter = data_adapter
        self.batch_size = batch_size
        self.embedding_service = AdvancedEmbeddingService()

    async def migrate_all_embeddings(self):
        """
        –ú–∏–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ embeddings –Ω–∞ transformer –≤–µ—Ä—Å–∏–∏

        –í–ù–ò–ú–ê–ù–ò–ï: –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –±–∞–∑
        """
        from datastorage.crud.datastorage import CRUDDataStorage
        from datastorage.database.models import SolutionPreprocessing

        ds = CRUDDataStorage(
            model=SolutionPreprocessing,
            session=self.data_adapter.session
        )

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏
        all_preps = await ds.list()

        print(f"üîÑ –ù–∞—á–∏–Ω–∞—é –º–∏–≥—Ä–∞—Ü–∏—é {len(all_preps)} embeddings...")

        migrated = 0
        errors = 0

        for i in range(0, len(all_preps), self.batch_size):
            batch = all_preps[i:i + self.batch_size]

            for prep in batch:
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
                    solution = prep.solution
                    if not solution:
                        continue

                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–æ–≤—ã–π embedding
                    new_embedding = self.embedding_service.generate_embedding(
                        solution.current_content
                    )

                    # –û–±–Ω–æ–≤–ª—è–µ–º
                    prep.embedding = json.dumps(new_embedding)
                    migrated += 1

                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –¥–ª—è {prep.solution_id}: {e}")
                    errors += 1

            # –ö–æ–º–º–∏—Ç–∏–º –±–∞—Ç—á
            await self.data_adapter.session.commit()
            print(
                f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {min(i + self.batch_size, len(all_preps))} / {len(all_preps)}")

        print(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {migrated} —É—Å–ø–µ—à–Ω–æ, {errors} –æ—à–∏–±–æ–∫")

        return {
            "total": len(all_preps),
            "migrated": migrated,
            "errors": errors
        }

    async def compare_quality(self, solution_id: str, top_k: int = 10):
        """
        –°—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã—Ö vs transformer embeddings

        –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞—Å–∫–æ–ª—å–∫–æ —É–ª—É—á—à–∏—Ç—Å—è –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞
        """
        # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–ª–µ–≤–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        solution = await self.data_adapter.get_solution(solution_id)
        if not solution:
            raise ValueError("–†–µ—à–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ–±–∞ —Ç–∏–ø–∞ embeddings
        text = solution.current_content

        # –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π (–∏–∑ improved_preprocessing)
        from .preprocessing_service import PreprocessingService
        lightweight_service = PreprocessingService(self.data_adapter)
        lightweight_emb = lightweight_service._generate_improved_embedding(text)

        # Transformer
        transformer_emb = self.embedding_service.generate_embedding(text)

        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥—Ä—É–≥–∏–µ —Ä–µ—à–µ–Ω–∏—è
        other_solutions = await self.data_adapter.get_other_solutions_for_challenge(
            solution.challenge_id, solution.user_id
        )

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º embeddings –¥–ª—è –≤—Å–µ—Ö
        other_texts = [s.current_content for s in other_solutions]

        lightweight_others = [
            lightweight_service._generate_improved_embedding(t)
            for t in other_texts
        ]

        transformer_others = self.embedding_service.generate_embeddings_batch(
            other_texts
        )

        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-K –¥–ª—è –∫–∞–∂–¥–æ–≥–æ
        def find_similar(query_emb, candidate_embs):
            query = np.array(query_emb)
            candidates = np.array(candidate_embs)
            similarities = np.dot(candidates, query) / (
                    np.linalg.norm(candidates, axis=1) * np.linalg.norm(query)
            )
            top_indices = np.argsort(similarities)[-top_k:][::-1]
            return [(int(idx), float(similarities[idx])) for idx in top_indices]

        lightweight_results = find_similar(lightweight_emb, lightweight_others)
        transformer_results = find_similar(transformer_emb, transformer_others)

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º
        print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ embeddings:")
        print("\n1Ô∏è‚É£ –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ (TF-IDF + n-grams):")
        for idx, score in lightweight_results[:5]:
            sol = other_solutions[idx]
            print(
                f"   {score:.3f} - {sol.id[:8]}... ({len(sol.current_content)} —Å–∏–º–≤–æ–ª–æ–≤)")

        print("\n2Ô∏è‚É£ Transformer (all-MiniLM-L6-v2):")
        for idx, score in transformer_results[:5]:
            sol = other_solutions[idx]
            print(
                f"{score:.3f} - {sol.id[:8]}... ({len(sol.current_content)} —Å–∏–º–≤–æ–ª–æ–≤)"
            )

        # –ú–µ—Ç—Ä–∏–∫–∏
        overlap = len(set([r[0] for r in lightweight_results]) &
                      set([r[0] for r in transformer_results]))

        print(f"\nüìà –ú–µ—Ç—Ä–∏–∫–∏:")
        print(
            f"–ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Ç–æ–ø-{top_k}: {overlap}/{top_k} ({overlap / top_k * 100:.1f}%)"
        )
        print(
            f"–°—Ä–µ–¥–Ω—è—è similarity (–ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ): {np.mean([s for _, s in lightweight_results]):.3f}"
        )
        print(
            f"–°—Ä–µ–¥–Ω—è—è similarity (transformer): {np.mean([s for _, s in transformer_results]):.3f}"
        )

        return {
            "lightweight_results": lightweight_results,
            "transformer_results": transformer_results,
            "overlap": overlap,
            "overlap_percent": overlap / top_k * 100
        }


# ============================================================================
# –ì–æ—Ç–æ–≤–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –¥–ª—è production
# ============================================================================

def create_embedding_service(use_transformers: bool = False):
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è embedding —Å–µ—Ä–≤–∏—Å–∞

    Args:
        use_transformers: –ï—Å–ª–∏ True - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç sentence-transformers

    Returns:
        –°–µ—Ä–≤–∏—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ embeddings
    """
    if use_transformers and SENTENCE_TRANSFORMERS_AVAILABLE:
        return AdvancedEmbeddingService()
    else:
        # Fallback –Ω–∞ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã–µ
        return None  # –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –º–µ—Ç–æ–¥ –∏–∑ ImprovedPreprocessingService


# Singleton –¥–ª—è transformer —Å–µ—Ä–≤–∏—Å–∞
_transformer_service_instance: Optional[AdvancedEmbeddingService] = None


def get_transformer_embedding_service() -> Optional[AdvancedEmbeddingService]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ singleton instance transformer —Å–µ—Ä–≤–∏—Å–∞"""
    global _transformer_service_instance

    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        return None

    if _transformer_service_instance is None:
        _transformer_service_instance = AdvancedEmbeddingService()

    return _transformer_service_instance
